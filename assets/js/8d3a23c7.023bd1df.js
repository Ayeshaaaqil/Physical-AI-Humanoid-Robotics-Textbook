"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8348],{8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var o=i(6540);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}},8775:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>a});const o=JSON.parse('{"id":"chapter-6-capstone-ai-robot-pipeline","title":"Chapter 6 - Capstone: Simple AI-Robot Pipeline","description":"Learning Objectives","source":"@site/docs/chapter-6-capstone-ai-robot-pipeline.md","sourceDirName":".","slug":"/chapter-6-capstone-ai-robot-pipeline","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/chapter-6-capstone-ai-robot-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/Ayeshaaaqil/Physical-AI-Humanoid-Robotics-Textbook/edit/main/docs/chapter-6-capstone-ai-robot-pipeline.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"chapter-6-capstone-ai-robot-pipeline","title":"Chapter 6 - Capstone: Simple AI-Robot Pipeline","sidebar_label":"6. Capstone Project","sidebar_position":6},"sidebar":"textbookSidebar","previous":{"title":"5. Vision-Language-Action (VLA)","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/chapter-5-vision-language-action-systems"}}');var t=i(4848),s=i(8453);const r={id:"chapter-6-capstone-ai-robot-pipeline",title:"Chapter 6 - Capstone: Simple AI-Robot Pipeline",sidebar_label:"6. Capstone Project",sidebar_position:6},l="Chapter 6: Capstone - Simple AI-Robot Pipeline",c={},a=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"System Architecture",id:"system-architecture",level:3},{value:"Design Principles",id:"design-principles",level:3},{value:"Example: Object Pick-and-Place Pipeline",id:"example-object-pick-and-place-pipeline",level:3},{value:"Practical Application",id:"practical-application",level:2},{value:"Project: Autonomous Object Sorter",id:"project-autonomous-object-sorter",level:3},{value:"Step 1: Perception - Object Detection",id:"step-1-perception---object-detection",level:4},{value:"Step 2: Cognition - Decision Making",id:"step-2-cognition---decision-making",level:4},{value:"Step 3: Planning - Motion Planning",id:"step-3-planning---motion-planning",level:4},{value:"Step 4: Control - Execution",id:"step-4-control---execution",level:4},{value:"Integration: Launch File",id:"integration-launch-file",level:3},{value:"Testing the Pipeline",id:"testing-the-pipeline",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Safety First",id:"1-safety-first",level:3},{value:"2. Robust Error Handling",id:"2-robust-error-handling",level:3},{value:"3. Modular Design",id:"3-modular-design",level:3},{value:"4. Testing Strategy",id:"4-testing-strategy",level:3},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-6-capstone---simple-ai-robot-pipeline",children:"Chapter 6: Capstone - Simple AI-Robot Pipeline"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Design an end-to-end AI-robot pipeline from perception to action"}),"\n",(0,t.jsx)(n.li,{children:"Integrate ROS 2, simulation, and AI models in a cohesive system"}),"\n",(0,t.jsx)(n.li,{children:"Apply best practices for robot software architecture"}),"\n",(0,t.jsx)(n.li,{children:"Deploy and test a complete robotic application"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(n.p,{children:["This capstone chapter brings together concepts from all previous chapters to build a complete ",(0,t.jsx)(n.strong,{children:"AI-robot pipeline"}),". You'll create a system that:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Perceives the environment with sensors"}),"\n",(0,t.jsx)(n.li,{children:"Processes data with AI models"}),"\n",(0,t.jsx)(n.li,{children:"Plans and executes actions"}),"\n",(0,t.jsx)(n.li,{children:"Operates safely in simulation and on real hardware"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This hands-on project demonstrates the full software stack for Physical AI systems."}),"\n",(0,t.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,t.jsx)(n.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(n.p,{children:"A typical AI-robot pipeline consists of four layers:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Perception Layer (Sensors \u2192 AI Models) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Cognition Layer (Decision Making)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Planning Layer (Motion & Task Planning)\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Control Layer (Actuator Commands)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h3,{id:"design-principles",children:"Design Principles"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Modularity"}),": Each component is an independent ROS 2 node"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness"}),": Handle sensor failures and unexpected inputs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety"}),": Emergency stop and collision avoidance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Testability"}),": Validate each layer independently"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"example-object-pick-and-place-pipeline",children:"Example: Object Pick-and-Place Pipeline"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": Detect an object, grasp it, and place it in a target location."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Pipeline"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception"}),": Camera detects object position"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cognition"}),": Decide whether object is graspable"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Planning"}),": Compute arm trajectory to reach object"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Control"}),": Execute trajectory and close gripper"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"practical-application",children:"Practical Application"}),"\n",(0,t.jsx)(n.h3,{id:"project-autonomous-object-sorter",children:"Project: Autonomous Object Sorter"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Build a robot that sorts colored blocks into bins."]}),"\n",(0,t.jsx)(n.h4,{id:"step-1-perception---object-detection",children:"Step 1: Perception - Object Detection"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass ObjectDetector(Node):\n    def __init__(self):\n        super().__init__('object_detector')\n        self.subscription = self.create_subscription(\n            Image, '/camera/image', self.image_callback, 10)\n        self.publisher = self.create_publisher(\n            DetectedObjects, '/detected_objects', 10)\n        self.bridge = CvBridge()\n\n    def image_callback(self, msg):\n        # Convert ROS Image to OpenCV format\n        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n        # Simple color-based detection (red blocks)\n        hsv = cv2.cvtColor(cv_image, cv2.COLOR_BGR2HSV)\n        lower_red = np.array([0, 100, 100])\n        upper_red = np.array([10, 255, 255])\n        mask = cv2.inRange(hsv, lower_red, upper_red)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL,\n                                       cv2.CHAIN_APPROX_SIMPLE)\n\n        # Publish detected objects\n        objects = DetectedObjects()\n        for contour in contours:\n            x, y, w, h = cv2.boundingRect(contour)\n            objects.objects.append(Object(x=x, y=y, color='red'))\n\n        self.publisher.publish(objects)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"step-2-cognition---decision-making",children:"Step 2: Cognition - Decision Making"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class TaskPlanner(Node):\n    def __init__(self):\n        super().__init__('task_planner')\n        self.subscription = self.create_subscription(\n            DetectedObjects, '/detected_objects', self.plan_callback, 10)\n        self.publisher = self.create_publisher(\n            TaskGoal, '/task_goal', 10)\n\n    def plan_callback(self, msg):\n        if len(msg.objects) == 0:\n            return  # No objects detected\n\n        # Pick closest object\n        closest_obj = min(msg.objects, key=lambda obj: obj.x**2 + obj.y**2)\n\n        # Determine target bin based on color\n        target_bin = self.get_bin_for_color(closest_obj.color)\n\n        # Publish task goal\n        goal = TaskGoal()\n        goal.action = 'pick_and_place'\n        goal.object_position = [closest_obj.x, closest_obj.y]\n        goal.target_position = target_bin\n        self.publisher.publish(goal)\n\n    def get_bin_for_color(self, color):\n        bins = {'red': [1.0, 0.0], 'blue': [-1.0, 0.0]}\n        return bins.get(color, [0.0, 0.0])\n"})}),"\n",(0,t.jsx)(n.h4,{id:"step-3-planning---motion-planning",children:"Step 3: Planning - Motion Planning"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from moveit_msgs.msg import MoveGroupActionGoal\n\nclass MotionPlanner(Node):\n    def __init__(self):\n        super().__init__('motion_planner')\n        self.subscription = self.create_subscription(\n            TaskGoal, '/task_goal', self.plan_motion, 10)\n        self.moveit_client = ActionClient(self, MoveGroup, 'move_group')\n\n    def plan_motion(self, goal):\n        # Convert object position to 3D coordinates\n        target_pose = self.pixel_to_world(goal.object_position)\n\n        # Create MoveIt goal\n        moveit_goal = MoveGroupActionGoal()\n        moveit_goal.request.group_name = 'manipulator'\n        moveit_goal.request.target_pose = target_pose\n\n        # Send goal to MoveIt\n        self.moveit_client.send_goal_async(moveit_goal)\n\n    def pixel_to_world(self, pixel_coords):\n        # Camera calibration: convert 2D pixel to 3D world coordinates\n        # Simplified example\n        x = pixel_coords[0] * 0.001  # Scale factor\n        y = pixel_coords[1] * 0.001\n        z = 0.5  # Fixed height above table\n        return [x, y, z]\n"})}),"\n",(0,t.jsx)(n.h4,{id:"step-4-control---execution",children:"Step 4: Control - Execution"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class RobotController(Node):\n    def __init__(self):\n        super().__init__('robot_controller')\n        self.joint_pub = self.create_publisher(\n            JointTrajectory, '/joint_trajectory', 10)\n        self.gripper_pub = self.create_publisher(\n            GripperCommand, '/gripper_command', 10)\n\n    def execute_trajectory(self, trajectory):\n        # Publish joint trajectory\n        self.joint_pub.publish(trajectory)\n\n    def close_gripper(self):\n        cmd = GripperCommand()\n        cmd.position = 0.0  # Fully closed\n        self.gripper_pub.publish(cmd)\n\n    def open_gripper(self):\n        cmd = GripperCommand()\n        cmd.position = 0.08  # Fully open\n        self.gripper_pub.publish(cmd)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"integration-launch-file",children:"Integration: Launch File"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Perception\n        Node(\n            package='my_robot',\n            executable='object_detector',\n            name='object_detector'\n        ),\n        # Cognition\n        Node(\n            package='my_robot',\n            executable='task_planner',\n            name='task_planner'\n        ),\n        # Planning\n        Node(\n            package='my_robot',\n            executable='motion_planner',\n            name='motion_planner'\n        ),\n        # Control\n        Node(\n            package='my_robot',\n            executable='robot_controller',\n            name='robot_controller'\n        ),\n        # Simulation\n        Node(\n            package='gazebo_ros',\n            executable='spawn_entity.py',\n            arguments=['-file', 'robot.urdf', '-entity', 'my_robot']\n        )\n    ])\n"})}),"\n",(0,t.jsx)(n.h3,{id:"testing-the-pipeline",children:"Testing the Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# 1. Launch Gazebo simulation\nros2 launch gazebo_ros gazebo.launch.py\n\n# 2. Launch the complete pipeline\nros2 launch my_robot object_sorter.launch.py\n\n# 3. Monitor system\nros2 node list\nros2 topic echo /detected_objects\nros2 topic echo /task_goal\n\n# 4. Visualize in RViz\nros2 run rviz2 rviz2\n"})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsx)(n.h3,{id:"1-safety-first",children:"1. Safety First"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Implement ",(0,t.jsx)(n.strong,{children:"emergency stop"}),": Hardware button to halt all motion"]}),"\n",(0,t.jsxs)(n.li,{children:["Add ",(0,t.jsx)(n.strong,{children:"collision detection"}),": Monitor force sensors and stop on contact"]}),"\n",(0,t.jsxs)(n.li,{children:["Use ",(0,t.jsx)(n.strong,{children:"velocity limits"}),": Cap maximum joint speeds"]}),"\n",(0,t.jsxs)(n.li,{children:["Test in ",(0,t.jsx)(n.strong,{children:"simulation"})," before deploying to hardware"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-robust-error-handling",children:"2. Robust Error Handling"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def safe_execute(self, action):\n    try:\n        result = self.execute_action(action)\n        return result\n    except Exception as e:\n        self.get_logger().error(f'Execution failed: {e}')\n        self.emergency_stop()\n        return None\n"})}),"\n",(0,t.jsx)(n.h3,{id:"3-modular-design",children:"3. Modular Design"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"One node = one responsibility"}),"\n",(0,t.jsx)(n.li,{children:"Use well-defined message interfaces"}),"\n",(0,t.jsx)(n.li,{children:"Avoid tight coupling between components"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"4-testing-strategy",children:"4. Testing Strategy"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unit tests"}),": Test individual nodes in isolation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integration tests"}),": Verify communication between nodes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System tests"}),": Run full pipeline in simulation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hardware tests"}),": Deploy to real robot incrementally"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Building an AI-robot pipeline requires integrating perception, cognition, planning, and control into a cohesive system. ROS 2 provides the framework for modular, scalable robotics software."}),"\n",(0,t.jsx)(n.p,{children:"This capstone project demonstrates how concepts from all previous chapters\u2014Physical AI, mechanics, ROS 2, simulation, and VLA models\u2014come together to create intelligent, autonomous systems."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Takeaways:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"AI-robot pipelines consist of perception, cognition, planning, and control layers"}),"\n",(0,t.jsx)(n.li,{children:"ROS 2 enables modular architecture with independent nodes"}),"\n",(0,t.jsx)(n.li,{children:"Safety and robustness are critical for real-world deployment"}),"\n",(0,t.jsx)(n.li,{children:"Test in simulation before deploying to hardware"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Project Ideas"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Autonomous navigation with obstacle avoidance"}),"\n",(0,t.jsx)(n.li,{children:"Visual servoing for precise manipulation"}),"\n",(0,t.jsx)(n.li,{children:"Multi-robot coordination for warehouse tasks"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Advanced Topics"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Behavior trees"})," for complex task planning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Predictive Control"})," for optimal trajectory execution"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-sensor fusion"})," with Kalman filters"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Open-Source Projects"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://moveit.ros.org/",children:"MoveIt 2"})," - Motion planning framework"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://navigation.ros.org/",children:"Navigation2"})," - Autonomous navigation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://github.com/ros-planning/moveit2_tutorials",children:"Manipulation"})," - Pick-and-place tutorials"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Communities"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://discourse.ros.org/",children:"ROS Discourse"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://physicalintelligence.slack.com/",children:"Physical AI Slack Groups"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://robotics.stackexchange.com/",children:"Robotics Stack Exchange"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Congratulations!"})," You've completed the Physical AI & Humanoid Robotics essentials course. You now have the foundational knowledge to design, build, and deploy AI-powered robotic systems."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Next Steps:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Build your own project using the concepts learned"}),"\n",(0,t.jsx)(n.li,{children:"Contribute to open-source robotics projects"}),"\n",(0,t.jsx)(n.li,{children:"Explore advanced topics in reinforcement learning and control theory"}),"\n",(0,t.jsx)(n.li,{children:"Join the Physical AI community and share your work"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);